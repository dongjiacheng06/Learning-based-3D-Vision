# Learning-based 3D Vision

[![GitHub stars](https://img.shields.io/github/stars/dongjiacheng06/Learning-based-3D-Vision?style=social)](https://github.com/dongjiacheng06/Learning-based-3D-Vision/stargazers)
[![License](https://img.shields.io/github/license/dongjiacheng06/Learning-based-3D-Vision)](./LICENSE)
[![PRs Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen)](./CONTRIBUTING.md)

A carefully curated list of outstanding works in **3D vision**, aiming to provide a one-stop resource for scholars, practitioners, and enthusiasts interested in 3D vision and its potentially exciting roles in future embodied intelligence / world perception and other areas.
<p align="center">
  <img src="assets/image.png" alt="Learning-based 3D Vision" width="100%" style="border-radius: 15px; box-shadow: 0 4px 24px rgba(0,0,0,.1); margin: 5px 0;">
</p>

*Photo Credit: [Gemini-Nano-Bananaüçå](https://aistudio.google.com/models/gemini-2-5-flash-image)*.
</div>

---

## News & Updates
Major updates and announcements are shown below. Scroll for full taxonomy and paper lists.

- [2026.01] Repo Launch ‚Äî Learning-based-3D-Vision is now live! See [CONTRIBUTING.md](./CONTRIBUTING.md) for how to contribute.
- [Ongoing] Community Contributions Welcome ‚Äî Submit papers via PR or open an issue.
- ‚≠ê [Ongoing] If you find this useful, please consider giving a star and sharing it with your research community!

---

## Categories
- [Learning-based 3D Vision](#learning-based-3d-vision)
  - [News \& Updates](#news--updates)
  - [Categories](#categories)
  - [Surveys](#surveys)
  - [End to End 3D Reconstruction](#end-to-end-3d-reconstruction)
  - [Online 3R/SLAM](#online-3rslam)
  - [4D Reconstruction](#4d-reconstruction)
  - [3D Generation](#3d-generation)
  - [3D Perception](#3d-perception)
  - [4D Perception](#4d-perception)
  - [3D Free Method](#3d-free-method)
  - [Related Analysis](#related-analysis)
  - [3D Foundation Models' Application](#3d-foundation-models-application)
  - [Acknowledgements](#acknowledgements)
  - [Citation](#citation)

> **Legend**  
> ‚≠êÔ∏è Recommended / Must-read  
---

## Surveys 
- "Advances in Feed-Forward 3D Reconstruction and View Synthesis: A Survey". [arXiv 2025.07](https://arxiv.org/abs/2507.14501)
- "3D Scene Generation: A Survey". [arXiv 2025.05](https://arxiv.org/abs/2505.05474v1)


## End to End 3D Reconstruction 
- [‚≠êÔ∏è] **CroCo**, "CroCo: Self-Supervised Pre-training for 3D Vision Tasks by Cross-View Completion". [arXiv 2022.10](https://arxiv.org/abs/2210.10716)
- [‚≠êÔ∏è] **DUSt3R**, "DUSt3R: Geometric 3D Vision Made Easy". [arXiv 2023.12](https://arxiv.org/abs/2312.14132)
- [‚≠êÔ∏è] **MVSplat**, "MVSplat: Efficient 3D Gaussian Splatting from Sparse Multi-View Images". [arXiv 2024.03](https://arxiv.org/abs/2403.14627)
- [‚≠êÔ∏è] **NopoSplat**, "No Pose, No Problem: Surprisingly Simple 3D Gaussian Splats from Sparse Unposed Images". [arXiv 2024.10](https://arxiv.org/abs/2410.24207)
- [‚≠êÔ∏è] **VGGT**, "On Geometric Understanding and Learned Data Priors in VGGT". [arXiv 2025.03](https://arxiv.org/abs/2503.11651)
- [‚≠êÔ∏è] **E-RayZer**, "E-RayZer: Self-supervised 3D Reconstruction as Spatial Visual Pre-training". [arXiv 2025.12](https://arxiv.org/abs/2512.10950)
- [‚≠êÔ∏è] **œÄ^3**, "œÄ^3: Permutation-Equivariant Visual Geometry Learning". [arXiv 2025.07](https://arxiv.org/abs/2507.13347v2)
- [‚≠êÔ∏è] **OmniVGGT**, "OmniVGGT: Omni-Modality Driven Visual Geometry Grounded Transformer". [arXiv 2025.11](https://arxiv.org/abs/2511.10560v1)
- [‚≠êÔ∏è] **MapAnything**, "MapAnything: Universal Feed-Forward Metric 3D Reconstruction". [arXiv 2025.09](https://arxiv.org/abs/2509.13414v2)

## Online 3R/SLAM
### Online 3R
- [‚≠êÔ∏è] **Spann3R**, "3D Reconstruction with Spatial Memory". [arXiv 2024.08](https://arxiv.org/abs/2408.16061)
- [‚≠êÔ∏è] **CUT3R**, "Continuous 3D Perception Model with Persistent State". [arXiv 2025.01](https://arxiv.org/abs/2501.12387)
- [‚≠êÔ∏è] **Point3R**, "Point3R: Streaming 3D Reconstruction with Explicit Spatial Pointer Memory". [arXiv 2025.07](https://arxiv.org/abs/2507.02863)
- [‚≠êÔ∏è] **StreamVGGT**, "Streaming 4D Visual Geometry Transformer". [arXiv 2025.07](https://arxiv.org/abs/2507.11539)
- [‚≠êÔ∏è] **XStreamVGGT**, "XStreamVGGT: Extremely Memory-Efficient Streaming Vision Geometry
Grounded Transformer with KV Cache Compression". [arXiv 2026.01](https://arxiv.org/abs/2601.01204v1)
- [‚≠êÔ∏è] **InfiniteVGGT**, "InfiniteVGGT: Visual Geometry Grounded Transformer for Endless Streams". [arXiv 2026.01](https://arxiv.org/abs/2601.02281v1)
- [‚≠êÔ∏è] **TTT3R**, "TTT3R: 3D RECONSTRUCTION AS TEST-TIME TRAINING". [arXiv 2025.10](https://arxiv.org/abs/2509.26645v3)
### SLAM
- [‚≠êÔ∏è] **SLAM-Former**, "SLAM-Former: Putting SLAM into One Transformer". [arXiv 2025.09](https://arxiv.org/abs/2509.16909v1)
- [‚≠êÔ∏è] **VGGT-SLAM**, "VGGT-SLAM: Dense RGB SLAM Optimized on the SL(4) Manifold". [arXiv 2025.09](https://arxiv.org/abs/2505.12549)


## 4D Reconstruction
- [‚≠êÔ∏è] **4D-VGGT**, "4D-VGGT: A General Foundation Model with SpatioTemporal Awareness for Dynamic Scene Geometry Estimation". [arXiv 2025.1](https://arxiv.org/abs/2511.18416)
- [‚≠êÔ∏è] **Gaussian-Flow**, "Gaussian-Flow: 4D Reconstruction with Dynamic 3D Gaussian Particle". [arXiv 2023.12](https://arxiv.org/abs/2312.03431)
- [‚≠êÔ∏è] **Shape of Motion**, "Shape of Motion: 4D Reconstruction from a Single Video". [arXiv 2024.7](https://arxiv.org/abs/2407.13764)
- [‚≠êÔ∏è] **St4RTrack**, "St4RTrack: Simultaneous 4D Reconstruction and Tracking in the World". [arXiv 2025.4](https://arxiv.org/abs/2504.13152)
- [‚≠êÔ∏è] **SplatFields**, "Neural Gaussian Splats for Sparse 3D and 4D Reconstruction". [arXiv 2024.9](https://arxiv.org/abs/2409.11211)
- [‚≠êÔ∏è] **L4GM**, "L4GM: Large 4D Gaussian Reconstruction Model". [arXiv 2024.7](https://arxiv.org/abs/2406.10324)

## 3D Generation
- [‚≠êÔ∏è] **Zero123++**, "Zero123++: a Single Image to Consistent Multi-view Diffusion Base Model"[arXiv 2023.10](https://arxiv.org/abs/2310.15110)
- [‚≠êÔ∏è] **LRM**, "LRM: Large Reconstruction Model for Single Image to 3D"[arXiv 2023.11](https://arxiv.org/abs/2311.04400)
- [‚≠êÔ∏è] **LGM**, "LGM: Large Multi-View Gaussian Model for High-Resolution 3D Content Creation". [arXiv 2024.02](https://arxiv.org/abs/2402.05054)
- [‚≠êÔ∏è] **TRELLIS**, "Structured 3D Latents for Scalable and Versatile 3D Generation". [arXiv 2024.12](https://arxiv.org/pdf/2412.01506)
- [‚≠êÔ∏è] **LYRA**, "LYRA: Generative 3D Scene Reconstruction via Video Diffusion Model Self-Distillation
". [arXiv 2025.09](https://arxiv.org/pdf/2509.19296v1)


## 3D Perception
- [‚≠êÔ∏è] **SAM3D**, "SAM3D: Segment Anything in 3D Scenes". [arXiv 2023.07](https://arxiv.org/abs/2306.03908)
- [‚≠êÔ∏è] **SAM3D**, "SAM3D: Segment Anything in 3D Scenes". [arXiv 2024.01](https://arxiv.org/abs/2306.02245v2)


## 4D Perception
- [‚≠êÔ∏è] **Trace Anything**, "Trace Anything: Representing Any Video in 4D via Trajectory Fields". [arXiv 2025.10](https://arxiv.org/abs/2510.13802)
- [‚≠êÔ∏è] **3AM**, "3AM: Segment Anything with Geometric Consistency in Videos". [arXiv 2026.01](https://arxiv.org/abs/2601.08831)
- [‚≠êÔ∏è] **OmniWorld**, "OmniWorld: A Multi-Domain and Multi-Modal Dataset for
4D World Modeling". [arXiv 2025.09](https://arxiv.org/abs/2509.12201v1)
- [‚≠êÔ∏è] **ViPE**, "ViPE: Video Pose Engine for 3D Geometric Perception
". [arXiv 2025.08](https://arxiv.org/abs/2508.10934)


## 3D Free Method
- [‚≠êÔ∏è] **LVSM**, "LVSM: A Large View Synthesis Model with Minimal 3D Inductive Bias". [arXiv 2410.17242](https://arxiv.org/abs/2410.17242)
- [‚≠êÔ∏è] **RayZer**, "RayZer: A Self-supervised Large View Synthesis Model". [arXiv 2505.00702](https://arxiv.org/abs/2505.00702)
- [‚≠êÔ∏è] **XFactor**, "True Self-Supervised Novel View Synthesis is Transferable". [arXiv 2510.13063](https://arxiv.org/abs/2510.13063)
- 
## Related Analysis
- [‚≠êÔ∏è] "How Much 3D Do Video Foundation Models Encode?". [arXiv 2025.07](https://arxiv.org/abs/2512.19949)
- [‚≠êÔ∏è] "What Is The Best 3D Scene Representation for
Robotics? From Geometric to Foundation Models". [arXiv 2025.12](https://arxiv.org/abs/2512.03422v1)
- [‚≠êÔ∏è] "On Geometric Understanding and Learned Data Priors in VGGT". [arXiv 2025.12](https://arxiv.org/abs/2512.11508)


## 3D Foundation Models' Application
- [‚≠êÔ∏è] "Video World Models with Long-term Spatial Memory". [arXiv 2025.06](https://arxiv.org/abs/2506.05284)


## Acknowledgements
This project has largely drawn on the following projects:
-  [Awesome-World-Models](https://github.com/knightnemo/Awesome-World-Models)
  
Huge shoutout the the authors for their awesome work.

## Citation
If you find this repository useful, please consider citing it:

```bibtex
@misc{learningbased3dvision,
  title        = {Learning-based 3D Vision: A curated list of representative works in learning-based 3D vision},
  author       = {Dong, Jiacheng, Huan Li and Contributors},
  howpublished = {\url{https://github.com/dongjiacheng06/Awesome-3D-papers}},
  year         = {2026}
}

